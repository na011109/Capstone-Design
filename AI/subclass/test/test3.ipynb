{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I can't believe we're really doing this, going into the enchanted forest.Yeah, it's been my dream since I was a kid.Your dream? I thought you were afraid\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 대화 입력\n",
    "conversation = (\n",
    "    \"I can't believe we're really doing this, going into the enchanted forest.\"\n",
    "    \"Yeah, it's been my dream since I was a kid.\"\n",
    "    \"Your dream? I thought you were afraid of magic.\"\n",
    "    \"I was, but dreams can change, can't they?\"\n",
    "    \"Sure, as long as you're ready for whatever comes next.\"\n",
    "    \"Are you saying there's something we should worry about?\"\n",
    "    \"You can never be too sure, especially when magic is involved.\"\n",
    "    \"But we have each other, right? That should count for something.\"\n",
    "    \"Absolutely, friendship is the most powerful magic of all.\"\n",
    "    \"Just promise me one thing: if things get tough, you won't leave me behind.\"\n",
    "    \"I promise, not only because of our friendship but also because we're stronger together.\"\n",
    "    \"Good, because we're about to cross the threshold, and there's no turning back.\"\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "inputs = tokenizer([conversation], max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=64, min_length=0, max_length=40)\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lch85\\miniconda3\\envs\\cap\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary_text': \"I can't believe we're really doing this, going into the enchanted forest.Yeah, it's been my dream since I was a kid.Your dream? I thought you were afraid of magic.I was, but dreams can change, can't they?Sure, as long as you're ready for whatever comes next.Right, we need to find the Heart of the Forest. It's said to grant a single wish to those who find it.The wish can be anything?Yes, but it comes with a price, and we must be willing to pay it.Well, if it's for a good cause,\"}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# ChatGPT 요약 모델 로드\n",
    "chatgpt_summary = pipeline(\"summarization\", model=\"facebook/bart-large\")\n",
    "\n",
    "# 대화 요약\n",
    "chatgpt_summary_result = chatgpt_summary(conversation)\n",
    "\n",
    "# 요약 출력\n",
    "print(chatgpt_summary_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "inputs = tokenizer([conversation], max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
